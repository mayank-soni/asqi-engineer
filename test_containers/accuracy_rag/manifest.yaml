name: "accuracy_rag"
version: "1.0"
description: "A test container for testing the accuracy for rag systems."

input_systems:
  - name: "system_under_test"
    type: "rag_api"
    required: true
    description: "The RAG system being tested"
  - name: "evaluator_system"
    type: "llm_api"
    required: false
    description: "LLM system for evaluating the system under test responses"

input_datasets:
  - name: "rag_accuracy_dataset"
    required: true
    type: "huggingface"
    description: "The dataset used for RAG accuracy testing, which includes generated question-answer pairs and context."
    features:
      - name: "question"
        type: "string"
        description: "The question to ask the RAG system."
      - name: "answer"
        type: "string"
        description: "The ground truth answer for evaluation."
      - name: "context"
        type: "string"
        description: "Retrieved context documents (can be single string or list)."
  - name: "rag_if_dataset"
    type: "huggingface"
    required: true
    description: "The dataset used for RAG instruction following testing, which includes questions augmented with instruction following prompts."
    features:
      - name: "transformed_question"
        type: "string"
        description: "The instruction-augmented question to ask the system."
      - name: "original_question"
        type: "string"
        description: "The original question before instruction augmentation."
      - name: "instruction_type"
        type: "string"
        description: "Type/category of instruction applied."
      - name: "instruction_type_metadata"
        type: "dict"
        description: "Additional metadata about the instruction (JSON string or dict)."

input_schema:
  - name: "max_rows"
    type: "integer"
    required: false
    description: "Maximum number of rows to process from each dataset for faster testing. Defaults to 20."


output_metrics:
  - name: "success"
    type: "boolean"
    description: "Whether the test execution completed successfully"
  - name: "results_dir"
    type: "string"
    description: "Directory where the results are written."
  - name: "answer_correctness"
    type: "float"
    description: "Average answer correctness score (0.0 to 1.0)"
  - name: "faithfulness"
    type: "float"
    description: "Average faithfulness score - measures how factually consistent the answer is with the context (0.0 to 1.0)"
  - name: "helpfulness"
    type: "float"
    description: "Average helpfulness score - measures how relevant and complete the response is (0.0 to 1.0)"
  - name: "retrieval_correctness"
    type: "float"
    description: "Average retrieval correctness score - measures how well the retrieved context matches the query (0.0 to 1.0)"
  - name: "hit@k"
    type: "float"
    description: "Mean Hit@k score (fraction of queries for which any reference was found in top-k retrieved contexts)."
  - name: "conditional_task_success"
    type: "float"
    description: "Average Conditional Task success score (0.0 to 1.0)"
  - name: "retrieval_gate_pass_rate"
    type: "float"
    description: "Base retrieval-gate pass rate (fraction of base accuracy dataset queries that passed retrieval gate)" 
  - name: "faithfulness_gate_pass_rate"
    type: "float"
    description: "Base faithfulness-gate pass rate (fraction of base accuracy dataset queries that passed faithfulness gate)"
  - name: "hard_gate_pass_rate"
    type: "float"
    description: "Base hard-gate pass rate (fraction of base accuracy dataset queries that passed all hard gates)"
  - name: "instruction_following"
    type: "float"   
    description: "Average instruction following score - measures how well the model follows instructions in the prompt (0.0 to 1.0)"

output_reports:
  - name: "detailed_report"
    type: "html"
    description: "A detailed HTML report of the test execution"