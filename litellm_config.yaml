model_list:
  - model_name: my-model
    litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
      model: openai/gpt-4o-mini ### MODEL NAME sent to `litellm.completion()` ###
      api_key: os.environ/OPENAI_API_KEY # does os.getenv("OPENAI_API_KEY")
  - model_name: claude-4-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY
  # Allow all OpenAI models
  - model_name: "openai/*"
    litellm_params:
      model: "openai/*"
      api_key: os.environ/OPENAI_API_KEY
  # Allow all bedrock models
  - model_name: "bedrock/*"
    litellm_params:
      model: "bedrock/*"
      api_key: os.environ/AWS_BEARER_TOKEN_BEDROCK
      drop_params: true # Some frameworks might send not supported params e.g. frequency_penalty
  - model_name: "custom_rag_chatbot"
    litellm_params:
      model: "custom_rag_chatbot"
      base_url: "http://host.docker.internal:7777" # Assuming the RAG API is running on the host machine at port 7777
      api_key: "anything"
      custom_llm_provider: rag_api_chatbot

litellm_settings:
  callbacks: ["otel"]
  custom_provider_map:
    - {
        "provider": "rag_api_chatbot",
        "custom_handler": "litellm_handlers.rag_chatbot_llm",
      }